---
layout: post
title: Qualitative Analysis
subtitle: Assessing the Dual-Model Performance 
comments: true
mathjax: true
---
For this study, we have used two different models–one to detect harmful loans ads using loss labels and the other to detect if the loan ads are fraudulent. Both the models operate on the textual content of advertisements and utilize pre-trained NLI models from the Hugging Face transformers library.

In the first model, we have used a multilingual zero-shot classification approach using the mDeBERTa-v3-base-mnli-xnli model to identify harmful loan ads based on loss-related risk categories in French. We first defined a set of loss-related risk categories in French, which reflect common patterns in deceptive or high-risk loan advertisements. These include: prêt instantané (instant loan),  promesses trompeuses d'argent rapide (misleading promises of fast money), taux annuel effectif global élevé (high APR), fausses subventions gouvernementales (fake government grants), arnaque au paiement anticipé (advance fee fraud), etc. The model evaluates each ad text against these categories in multi-label mode, allowing multiple risk indicators to be scored simultaneously. The model then returns a probability score for each of the predefined loss labels. If the score exceeded a threshold of 0.6, the ad was flagged as loss-related (is_loss = 1) and the corresponding loss_label was recorded. This approach allowed for flexible classification against custom labels without the need for task-specific fine-tuning.

In the second model, we have used a transformer-based zero-shot classification model, namely facebook/bart-large-mnli model, to classify the loan advertisements into “fraud” and “non-fraud” categories. The model assessed each advertisement’s body text and provided the score associated with the "fraud" label. If fraud score was more than 0.5, the ad was flagged as potentially fraudulent (fraud = 1). Otherwise, it was labeled as non-fraudulent (fraud = 0). 

## Assessment of model’s performance
To assess the performance of both the models in labelling loss-related loan advertisement and detecting fraudulent ads, we constructed two balanced evaluation datasets for human annotation. These datasets were used for comparison between the models' automated predictions and human judgments, thereby facilitating error analysis and validation of model reliability. For both the models, each dataset was split into two groups based on the binary model prediction: For model 1, this meant distinguishing between is_loss == 1 and is_loss == 0 and for model 2, the split was based on fraud == 1 and fraud == 0. 
We randomly sampled up to 50 ads from each class, to manually verify if the model’s predictions were correct. To assess the performance of the loss-related content detection model (model 1), we conducted a two-tiered evaluation–first to test the model’s classification accuracy (presence of loss-related risk) and its ability to correctly predict the specific type of loss. For the latter, we used a  multiclass evaluation to compare the model-predicted loss-label and the human-corrected labels.
We then constructed a confusion matrix to compare the model’s predictions with human-annotated ground truth. This allowed us to isolate and analyze errors in loss category identification while excluding irrelevant (non-loss) samples. The confusion matrix (Figure 9) indicates that model 1 performs robustly in distinguishing loss-related advertisements from others. The low false positive rate (only 1 case) demonstrates that the model is conservative in flagging content, while the moderate false negative rate (3 missed cases) suggests some caution is still needed when relying solely on automated predictions. 

![Figure 9](/assets/img/Figure 9.jpg){: .mx-auto.d-block :}
<center> Figure 9. The confusion matrix of the binary classification performance of Model 1 in identifying whether an ad is loss-related or not loss-related </center>

Overall, the model correctly predicted the label for 91% of the ads. It correctly predicted the loss label for 93% of the ads, while it detected 82% of actual loss-related ads. The relatively low false positive rate indicates a conservative tendency in predicting loss-related content, while the moderate false negative rate suggests that some genuine cases of loss-related content were missed. Overall, the F1 score of 87.18% reflects a balanced performance, suitable for scenarios where both precision and recall are important.

During the human annotation process, we observed that the model is able to correctly identify most financial risk indicators when they are explicitly stated in the advertisement text. For instance, categories such as “instant loan” and “fake loan guarantees” are typically recognized and labeled correctly. However, the model encounters difficulty when dealing with ads that use vague or indirect language. For example, phrases like “quick approval,” “easy financing,” or “no credit check.” These less specific expressions often lead to confusion in classification. In some cases, the model also mistakenly labels content as “currency risk,” which may reflect a limited understanding of financial terminology related to foreign exchange.

Similarly, we constructed a manually annotated dataset consisting of a stratified sample of advertisement texts previously labeled as "fraud" or "not fraud" by the model to evaluate the accuracy of the fraud detection model (model 2). After cleaning the data, we divided it into two groups based on the model's predictions: ads predicted to be fraudulent (fraud == 1) and ads predicted as non-fraudulent (fraud == 0). We randomly sampled up to 50 ads from each class, to manually verify if the model’s predictions were correct (1 for fraud, 0 for not fraud) and to check whether the model prediction agreed with the human judgment.
As shown in the confusion matrix (Figure 2), the model demonstrated competent performance in binary classification of fraudulent advertisements. The model achieved an overall accuracy of 80%, with a high precision of 90% for detecting fraudulent content. However, a recall of 75% indicates that approximately one-quarter of actual fraudulent ads were missed. This trade-off suggests that model 2 errs slightly on the side of caution, minimizing false alarms while accepting some detection gaps. The F1 score of 81.8% confirms a generally effective balance between precision and recall.

![Figure 10](/assets/img/Figure 10.jpg){: .mx-auto.d-block :}
<center> Figure 10. Figure 10. The confusion matrix Model 2, which classifies ads as either “fraud” or “not fraud” </center>

This aligns with observations during the human annotation process, wherein, the model’s accuracy declined while analyzing texts with subtle or ambiguous language. This is particularly evident in loan ads involving “fake loan guarantees,” where the advertisements may imply misleading benefits without making direct fraudulent claims. As a result, the model overlooked such fraud-related content.

Overall, the model demonstrates a reliable performance on well-structured and explicit loan advertisements. However, its generalization ability remains limited when facing complex or ambiguous language.
