---
layout: post
title: full page
subtitle: There's lots to learn!
gh-repo: daattali/beautiful-jekyll
gh-badge: [star, fork, follow]
comments: true
mathjax: true
---

{: .box-success}
## Introduction 
Advertising platforms such as Meta have become key channels through which users access financial service information, including loans, insurance, and credit scoring. According to Meta’s own data, more than two billion people use Facebook every month, making it a vital platform for businesses to reach potential customers. Meta operates a closed advertising ecosystem that serves only its own platform users, claiming to support advertisers throughout the entire customer lifecycle and thereby increasing marketing efficiency.
 
However, this advertising infrastructure is far from neutral. Meta’s ad system allows advertisers to narrow their target audience by attributes such as interests, gender, and location. Its algorithms prioritize users who are predicted to be more likely to engage with the ad. While this personalization ostensibly improves efficiency, it also has the potential to systematically shape who has access to what kind of information, raising concerns about structural inequality.
 
When audience targeting includes sensitive demographic or behavioral data, the risk of discrimination increases. Studies have shown that even when advertisers specify broad or neutral targeting parameters, the platform’s delivery algorithm can lead to skewed distribution across demographic lines. This is especially problematic in socially sensitive domains such as housing, employment, and financial services, where such biases may violate anti-discrimination laws. For instance, a 2016 ProPublica investigation revealed that Facebook has enabled advertisers to exclude users based on “ethnic affinities” from seeing housing ads. The U.S. Department of Housing and Urban Development later filed a lawsuit against Facebook, accusing the platform of “encouraging, enabling, and causing” discriminatory practices.
 
In the European context, these concerns fall under the regulatory scope of the General Data Protection Regulation (GDPR), which grants users the right to be informed about how their data is used for personalized advertising (Articles 13 and 14). GDPR also mandates that personal data must be processed lawfully, fairly, and transparently (Article 5). If a platform uses opaque optimization mechanisms to influence ad exposure without informing users, particularly in the context of financial products, it may be in breach of these principles.
 
More critically, algorithmic bias in ad delivery not only leads to unequal access to information, but may also exacerbate financial risk. Research shows that products such as payday loans and high-interest credit offers are disproportionately shown to women and lower-income users. Due to disparities in financial literacy, digital access, and representation in training data, women are often more vulnerable to being steered toward riskier financial products. These patterns reflect not only gender bias but also a potential amplification of existing economic precarity through algorithmic decision-making.
 
Against this backdrop, our study focuses on the French context to explore whether Meta’s ad delivery system exhibits structural bias based on gender and age in the distribution of loan-related advertising. Specifically, we ask: Are certain demographic groups unintentionally more frequently exposed to financial product advertisements that carry risk—and is this pattern driven by the platform’s own delivery algorithms?



## 1. Literature review
In today’s increasingly complex digital financial environment, the boundary between financial fraud and financial risk is becoming increasingly blurred. These two issues often overlap, posing significant threats to consumer financial security and undermining market fairness. In academic literature, financial fraud is commonly defined as a deliberate act of deception aimed at obtaining illicit gains through the manipulation of information or exploitation of system vulnerabilities (Button et al., 2014). However, there remains no consistent academic consensus on how to define specific types of fraud. As a result, the classification and identification of financial risks in practice are often shaped by the categorizations and warnings issued by government regulatory agencies. 

For instance, the New York State Department of Financial Services classifies high-risk financial products into several types of predatory loans and loan scams, including payday loans, tax refund anticipation loans, advance fee loan scams, and government grant loan scams. These products are frequently marketed using language such as “instant approval” or “no credit check,” yet they are commonly associated with extremely high annual percentage rates (APRs), false promises, identity theft, and upfront payment scams. Similarly, the European Financial Inclusion Network Working Group on Over-Indebtedness (2016) defines unfair lending practices as those that involve a lack of creditworthiness assessment or inadequate disclosure of key terms, and identifies payday loans and foreign currency loans as “toxic loans” due to their high costs and currency fluctuation risks, which can easily trap borrowers in long-term cycles of default.

In terms of risk identification and consumer protection, the U.S. Consumer Financial Protection Bureau (CFPB) highlights the need to be especially cautious of high-risk features hidden in loan agreements, such as prepayment penalties, balloon payments, negative amortization, and interest-only structures. These features may lead to unexpected costs during the loan term and increase the likelihood of borrower default. The CFPB advises borrowers to inquire about the necessity of these features and request alternative loan offers without such terms in order to compare costs and assess risk more accurately (CFPB, n.d.).

In the field of financial risk and fraud detection, machine learning techniques have been widely used to identify fraudulent behavior in large-scale datasets. Almarhabi et al. (2023) applied supervised machine learning methods—including Random Forest, Logistic Regression, and Naive Bayes to detect cryptocurrency fraud in the U.S. context, with the Random Forest model achieving near-perfect accuracy. However, their approach heavily relies on pre-labeled data and is therefore limited to detecting known fraud patterns. It struggles to identify novel or more covert forms of fraud. Moreover, the study focused solely on structured transactional data and did not address natural language processing (NLP) tasks, making it unsuitable for analyzing risky language in advertising or social media, especially in multilingual contexts.

Similarly, Rana et al. (2022) employed a pre-trained BERT language model to detect fraudulent job postings and achieved outstanding performance under a supervised learning setting (with an F1-score of 0.93). While their model benefited from BERT’s strong contextual understanding, it still required extensive human-labeled data for fine-tuning, which limits its applicability to emerging or subtle fraud schemes. Furthermore, since their dataset consisted of structured recruitment texts, the model lacked adaptability to unstructured advertising content and did not support multilingual analysis—factors essential to our study on cross-language ad-based risk detection.

Kuo and Tsang (2024) proposed a hybrid approach combining emotion analysis with supervised learning to identify investment scams. They extracted emotional fluctuation features across different scam lifecycle stages from Chinese chatroom data and applied classifiers such as Support Vector Machine, Decision Tree, and Random Tree. Although their findings demonstrated a strong correlation between emotional trajectories and scam-related language, the model’s dependence on a predefined Chinese emotion lexicon and structured dialogue data limits its ability to generalize to multilingual, semantically complex advertising texts. As such, it is not applicable to our research objectives.

By contrast, the zero-shot classification pipeline, a cutting-edge method in NLP, offers a promising solution to the challenge of limited labeled data (Yin et al., 2019). As an advanced form of deep transfer learning, it enables cross-task text classification without requiring additional supervised training, by leveraging language and task knowledge learned through Natural Language Inference (NLI). In this study, we adopt Microsoft’s multilingual mDeBERTa model (released in 2021), which was pre-trained on CC100 and fine-tuned on MNLI and XNLI datasets. It supports semantic reasoning across more than 100 languages, making it well-suited to the analysis of digital ads in highly multilingual environments.

We apply this zero-shot pipeline to analyze loan-related advertisements from Meta’s ad library, detecting financial risk content without the need for labeled training data. Its premise–hypothesis structure mimics human reasoning and aligns with value-based analytical frameworks commonly used in the social sciences. This makes it especially effective for identifying value-laden textual content such as financial scams, misleading promotions, and emotional manipulation. Moreover, the model’s adaptability to diverse languages and complex text formats positions it as a highly efficient and flexible tool for financial risk detection across low-resource, heterogeneous digital media platforms.


## 2. Methodology
This study aims to identify advertisements related to financial services that could potentially lead to monetary losses for targeted social media users. We first analyze textual ad content using a multilingual zero-shot classification model. The methodology is designed to detect specific themes related to financial deception, enabling a systematic labeling of ad data based on semantic content. Our primary objective is to first semantically categorize each advertisement based on predefined fraud-related themes and then analyse the demographic distribution of fraudulent ads. 

For the purpose of this study, we have used data from the META ads database, which contains advertisements, including text content, demographic details of users who were exposed to the advertisement, delivery start dates, and advertiser names. We have narrowed down the scope of our study to France in 2024 October. 

**Data Collection and Preprocessing**

Using API, a large amount of data was retrieved from the META Ad Library. From this, we filtered out the data for advertisements related to loans in France in the month of October 2024. To filter these ads, we used words such as 'loan', 'prêt', 'crédit', 'financement', 'microcrédit', 'emprunt' as the word embeddings in the 'ad_creative_bodies'. We then use a zero-shot classification to determine the probability of “loan” and filter out the ones with a probability of less than 0.5. A zero-shot classification is a machine learning technique where a model can classify data into categories without having seen any labeled examples of those categories during training. This model uses natural language inference (NLI) or semantic similarity to assess whether a given input text implies a particular label or category. For this study, we have used the MoritzLaurer/mDeBERTa-v3-base-mnli-xnli model, which is a transformer-based architecture fine-tuned for NLI across multiple languages. 

The data used for conducting a distribution analysis in our final stage to determine the exposure of different age groups to harmful loan data is also determined by the model, where in, the dummy variables for ads related to financial losses and the variable for determining if the advertisement is related to loans is positive. 

To accommodate for memory constraints, the data is processed incrementally in fixed-size chunks of 100 rows. This facilitates checkpointing and resumption of execution, thus ensuring fault tolerance and reducing memory overhead. 

Classification of loan ads

We have used a list of 11 fraud-related thematic labels that was manually curated in French.
These labels were selected to reflect common patterns in financial fraud and deceptive advertising. The classification process is designed such that each advertisement's textual body passes through a zero-shot classification pipeline with the labels mentioned above. The pipeline outputs a probability of 0 or 1 for each label, indicating the degree to which the text is semantically aligned with that concept. The label with the highest score is recorded as the primary category. To ensure safeguards against missing or malformed data, the model assigns zero scores to all labels where advertisement text is empty or the classification process fails. Once again, to enhance scalability and fault tolerance, processed rows are saved to disk after every 300 entries as separate chunk files. This strategy facilitates resumable execution, reduces memory usage, and mitigates the impact of interruptions during long processing sessions. 

The second stage of this study focuses on analyzing the distribution and targeting of potentially harmful advertisements across demographic groups, with an emphasis on age and gender. For the data analysis part, we first group advertisements that convey highly similar or semantically equivalent content. We achieve this by using textual similarity measures to cluster ads that share core messaging—particularly those related to financial services or loan offers. By clustering these ads, we aim to identify whether content labeled as leading to financial loss is more frequently delivered to specific user profiles. In particular, we investigate the distribution of the labels across different age groups. This enables us to trace how the algorithm potentially reuses or adapts fraudulent messages across user segments. Such patterns may suggest algorithmic targeting behaviors that disproportionately affect certain demographic profiles.

Our dataset includes aggregated exposure data broken down by age and gender categories for each ad. So, to accurately interpret the exposure of different demographic groups to these potentially harmful ads, we calculate the overall demographic distribution of ad impressions across all ads. This includes the proportion of views from each age–gender group, such as males aged 18–24, females aged 45–54, etc. However, we normalize the demographic data because the number of users in each demographic group would vary and a direct comparison of exposure levels would be misleading. 

To evaluate potential gender- and age-based biases in the targeting of harmful loan advertisements, we calculate a normalized exposure ratio that adjusts for general exposure to loan-related content across demographic groups. Specifically, for each demographic group g (e.g., females aged 18–24), we define:

D_loss[g] as the average percentage representation of group g in loss-inducing loan ads (i.e., ads where is_loss = 1),

D_loan[g] as the average percentage representation of group g in all loan ads (i.e., where is_loan = 1).

The normalized exposure for group g is then given by:

NormalizedExposure_g = D_loss[g] / D_loan[g]

This ratio measures the relative concentration of harmful content within each demographic group’s exposure to loan advertising. A value of NormalizedExposure_g > 1 indicates that group g is overrepresented in harmful loan ads compared to their baseline exposure—suggesting a disproportionate targeting. In contrast, a value below 1 signals underrepresentation, implying that the group is less likely to be exposed to risky or deceptive financial advertisements relative to their general presence in loan ad audiences. This normalization enables a bias-aware assessment of risk distribution, highlighting potential disparities in exposure to financial harm across demographic lines. In simple terms, it allows us to account for disparities in group sizes and ensures that comparisons of exposure are not biased by the natural prevalence of certain user segments. 
While this methodology is robust, it has several limitations. In this methodology, the fraud detection relies entirely on the semantic alignment of ad text with predefined labels, while  contextual or multimedia cues are not taken into consideration. So, language ambiguity and informal phrasing in ad texts may reduce classification accuracy, particularly for underrepresented dialects or idiomatic expressions. It is important to note that zero-shot models are not trained directly on the classification task at hand. Instead, they rely on general-purpose reasoning to approximate categorization. Thus, the accuracy of this model will be less compared to models fine-tuned on labeled examples specific to the domain (such as financial fraud detection). Moreover, even if the model is multilingual, its performance can vary widely between languages. In addition, the model may not be optimal across labels and lead to false positives and false negatives due to the absence of a fixed probability threshold.




[This is a link to a different site](https://deanattali.com/) and [this is a link to a section inside this page](#local-urls).

Here's a table:

| Number | Next number | Previous number |
| :------ |:--- | :--- |
| Five | Six | Four |
| Ten | Eleven | Nine |
| Seven | Eight | Six |
| Two | Three | One |

You can use [MathJax](https://www.mathjax.org/) to write LaTeX expressions. For example:
When \\(a \ne 0\\), there are two solutions to \\(ax^2 + bx + c = 0\\) and they are $$x = {-b \pm \sqrt{b^2-4ac} \over 2a}.$$

How about a yummy crepe?

![Crepe](https://beautifuljekyll.com/assets/img/crepe.jpg)

It can also be centered!

![Crepe](https://beautifuljekyll.com/assets/img/crepe.jpg){: .mx-auto.d-block :}

Here's a code chunk:

~~~
var foo = function(x) {
  return(x + 5);
}
foo(3)
~~~

And here is the same code with syntax highlighting:

```javascript
var foo = function(x) {
  return(x + 5);
}
foo(3)
```

And here is the same code yet again but with line numbers:

{% highlight javascript linenos %}
var foo = function(x) {
  return(x + 5);
}
foo(3)
{% endhighlight %}

## Boxes
You can add notification, warning and error boxes like this:

### Notification

{: .box-note}
**Note:** This is a notification box.

### Warning

{: .box-warning}
**Warning:** This is a warning box.

### Error

{: .box-error}
**Error:** This is an error box.

## Local URLs in project sites {#local-urls}

When hosting a *project site* on GitHub Pages (for example, `https://USERNAME.github.io/MyProject`), URLs that begin with `/` and refer to local files may not work correctly due to how the root URL (`/`) is interpreted by GitHub Pages. You can read more about it [in the FAQ](https://beautifuljekyll.com/faq/#links-in-project-page). To demonstrate the issue, the following local image will be broken **if your site is a project site:**

![Crepe](/assets/img/crepe.jpg)

If the above image is broken, then you'll need to follow the instructions [in the FAQ](https://beautifuljekyll.com/faq/#links-in-project-page). Here is proof that it can be fixed:

![Crepe]({{ '/assets/img/crepe.jpg' | relative_url }})

<details markdown="1">
<summary>Click here!</summary>
Here you can see an **expandable** section
</details>
